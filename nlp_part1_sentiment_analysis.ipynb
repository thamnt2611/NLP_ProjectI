{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Phân tích văn bản thành Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "def load_vndoc(filename):\n",
    "    file = open(filename, 'r', encoding=\"utf-8\")\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm 1.5 meters tall. What about you? Don't say you are higher than me.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text = \"I'm 1.5 meters tall. What about you? Don't say you are higher than me.\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm\", '1', '5', 'meters', 'tall', 'What', 'about', 'you', \"Don't\", 'say', 'you', 'are', 'higher', 'than', 'me', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "tokens = re.split(r'[-\\s.,;!?)(]+', text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 RegexpTokenizer (English & Vietnamese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", '1', '.5', 'meters', 'tall', '.', 'What', 'about', 'you', '?', 'Don', \"'t\", 'say', 'you', 'are', 'higher', 'than', 'me', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer \n",
    "tokenizer_2 = RegexpTokenizer(r'\\w+|$[0-9.]+|\\S+')\n",
    "token_2 = tokenizer_2.tokenize(text)\n",
    "print(token_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", '1.5', 'meters', 'tall.', 'What', 'about', 'you', '?', 'Do', \"n't\", 'say', 'you', 'are', 'higher', 'than', 'me', '.']\n"
     ]
    }
   ],
   "source": [
    "# better\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer_3 = TreebankWordTokenizer()\n",
    "print(tokenizer_3.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 ```casual_tokenize```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wowwww', '!', 'I', \"can't\", 'believe', 'you', 'can', 'do', 'that', '!', 'Suprising', '!', 'Suhhhhhhhhhhh', '!', 'Keep', 'silent', 'plez', '!']\n",
      "['Wowwww', '!', 'I', 'ca', \"n't\", 'believe', 'you', 'can', 'do', 'that', '!', 'Suprising', '!', 'Suhhhhhhhhhhh', '!', 'Keep', 'silent', 'plez', '!']\n",
      "['Wowww', '!', 'I', \"can't\", 'believe', 'you', 'can', 'do', 'that', '!', 'Suprising', '!', 'Suhhh', '!', 'Keep', 'silent', 'plez', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.casual import casual_tokenize\n",
    "message = \"Wowwww! I can't believe you can do that! Suprising! Suhhhhhhhhhhh! Keep silent plez!\"\n",
    "print(casual_tokenize(message))\n",
    "print(tokenizer_3.tokenize(message))\n",
    "print(casual_tokenize(message, reduce_len=True, strip_handles = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 pyvi.ViTokenizer cho Tiếng Việt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['USD', 'điều_chỉnh', 'trái', 'chiều', ',', 'vàng', 'SJC', 'quay', 'đầu', 'tăng', 'Mỗi', 'lượng', 'vàng', 'SJC', 'sáng', 'nay', 'tăng', '20.000', 'đồng', 'sau', 'khi', 'vừa', 'có', 'phiên', 'đi', 'xuống', ';', 'tỷ_giá', 'trung_tâm', 'được', 'Ngân_hàng', 'Nhà_nước', 'tăng', '10', 'đồng', 'nhưng', 'các', 'nhà', 'băng', 'lại', 'giảm', '.', 'Mở_cửa', 'đầu', 'ngày', '23', '/', '11', ',', 'Tập_đoàn', 'DOJI', 'niêm_yết_giá', 'vàng', 'miếng', 'SJC', 'cao', 'hơn', '20.000', 'đồng', 'một', 'lượng', 'so', 'với', 'hôm_qua', 'với', 'giá', 'mua_bán', '36,44', '-', '36,54', 'triệu', 'đồng', '.', 'Tương_tự', ',', 'các', 'doanh_nghiệp', 'vàng', 'khác', 'trong', 'nước', 'cũng', 'tăng', 'vài', 'chục', 'nghìn', 'đồng', 'một', 'lượng', 'và', 'dao_động', 'quanh', 'mốc', '36,5', 'triệu', 'đồng', '.', 'Giao_dịch', 'vàng', 'tại', 'doanh_nghiệp', 'PNJ.', 'Ảnh', ':', 'Lệ_Chi', '.', 'Giao_dịch', 'vàng', 'tại', 'doanh_nghiệp', 'PNJ.', 'Ảnh', ':', 'Lệ_Chi', '.', 'Sự', 'đi', 'lên', 'nhẹ', 'của', 'giá', 'vàng', 'trong', 'nước', 'cũng', 'chung', 'xu_hướng', 'với', 'thế_giới', '.', 'Theo', 'đó', ',', 'chốt', 'phiên', 'Mỹ', 'tối', 'qua', ',', 'mỗi', 'ounce', 'tăng', 'gần', '2', 'USD', ',', 'lên', '1.227', 'USD', 'một', 'ounce', '.', 'Sang', 'phiên', 'Châu', 'Á', 'sáng', 'nay', ',', 'kim_loại', 'quý', 'vẫn', 'dao_động', 'quanh', 'mốc', 'này', '.', 'Quy_đổi', 'sang', 'tiền', 'Việt', ',', 'giá', 'thế_giới', 'hiện', 'tương_đương', '34,6', 'triệu', 'đồng', 'một', 'lượng', '(', 'chưa', 'thuế', ',', 'phí', ',', 'gia_công', ')', '.', 'Chênh_lệch', 'với', 'thị_trường', 'trong', 'nước', 'vẫn', 'quanh', '1,9', 'triệu', 'đồng', '.', 'Tập_đoàn', 'DOJI', 'cho', 'biết', ',', 'trên', 'thị_trường', 'ghi_nhận', 'lượng', 'khách', 'bán', 'vàng', 'ra', 'chiếm', 'chủ_đạo', ',', 'với', 'tỷ_lệ', 'khoảng', '65', '%', 'trên', 'tổng_lượng', 'giao_dịch', 'tại', 'doanh_nghiệp', '.', 'Trên', 'thị_trường', 'ngoại_hối', ',', 'tỷ_giá', 'trung_tâm', 'do', 'Ngân_hàng', 'Nhà_nước', 'công_bố', 'sáng', 'nay', 'là', '22.743', 'đồng', 'một', 'USD', ',', 'tăng', '10', 'đồng', 'so', 'với', 'hôm_qua', '.', 'Trong', 'khi', 'đó', ',', 'giá', 'USD', 'tại', 'các', 'ngân_hàng', 'thương_mại', 'lại', 'đi', 'xuống', '.', 'Cụ_thể', ',', 'giá', 'bán', 'đôla', 'Mỹ', 'tại', 'Vietcombank', 'hiện', 'quanh', '23.295', '-', '23.385', 'đồng', ',', 'giảm', '10', 'đồng', 'so', 'với', 'hôm_qua', '.']\n"
     ]
    }
   ],
   "source": [
    "from pyvi import ViTokenizer\n",
    "text_vn = load_vndoc('D:/Documents/CODEBOOK/Course/text_vn_1.txt')\n",
    "print(ViTokenizer.tokenize(text_vn).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 underthesea.word_tokenize cho Tiếng Việt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['USD', 'điều chỉnh', 'trái', 'chiều', ',', 'vàng', 'SJC', 'quay', 'đầu', 'tăng', 'Mỗi', 'lượng', 'vàng', 'SJC', 'sáng', 'nay', 'tăng', '20.000', 'đồng', 'sau', 'khi', 'vừa', 'có', 'phiên', 'đi', 'xuống', ';', 'tỷ giá', 'trung tâm', 'được', 'Ngân hàng', 'Nhà nước', 'tăng', '10', 'đồng', 'nhưng', 'các', 'nhà băng', 'lại', 'giảm', '.', 'Mở cửa', 'đầu', 'ngày', '23/11', ',', 'Tập đoàn', 'DOJI', 'niêm yết giá', 'vàng', 'miếng', 'SJC', 'cao', 'hơn', '20.000', 'đồng', 'một', 'lượng', 'so', 'với', 'hôm qua', 'với', 'giá', 'mua bán', '36,44', '-', '36,54', 'triệu', 'đồng', '.', 'Tương tự', ',', 'các', 'doanh nghiệp', 'vàng', 'khác', 'trong', 'nước', 'cũng', 'tăng', 'vài', 'chục', 'nghìn', 'đồng', 'một', 'lượng', 'và', 'dao động', 'quanh', 'mốc', '36,5', 'triệu', 'đồng', '.', 'Giao dịch', 'vàng', 'tại', 'doanh nghiệp', 'PNJ. Ảnh', ':', 'Lệ Chi', '.', 'Giao dịch', 'vàng', 'tại', 'doanh nghiệp', 'PNJ. Ảnh', ':', 'Lệ Chi', '.', 'Sự', 'đi', 'lên', 'nhẹ', 'của', 'giá', 'vàng', 'trong', 'nước', 'cũng', 'chung', 'xu hướng', 'với', 'thế giới', '.', 'Theo', 'đó', ',', 'chốt', 'phiên', 'Mỹ', 'tối qua', ',', 'mỗi', 'ounce', 'tăng', 'gần', '2', 'USD', ',', 'lên', '1.227', 'USD', 'một', 'ounce', '. Sang', 'phiên', 'Châu Á', 'sáng', 'nay', ',', 'kim loại', 'quý', 'vẫn', 'dao động', 'quanh', 'mốc', 'này', '.', 'Quy đổi', 'sang', 'tiền', 'Việt', ',', 'giá', 'thế giới', 'hiện', 'tương đương', '34,6 triệu', 'đồng', 'một', 'lượng', '(', 'chưa', 'thuế', ',', 'phí', ',', 'gia công', ')', '.', 'Chênh lệch', 'với', 'thị trường', 'trong', 'nước', 'vẫn', 'quanh', '1,9', 'triệu', 'đồng', '.', 'Tập đoàn', 'DOJI', 'cho', 'biết', ',', 'trên', 'thị trường', 'ghi nhận', 'lượng', 'khách', 'bán', 'vàng', 'ra', 'chiếm', 'chủ đạo', ',', 'với', 'tỷ lệ', 'khoảng', '65', '%', 'trên', 'tổng lượng', 'giao dịch', 'tại', 'doanh nghiệp', '.', 'Trên', 'thị trường', 'ngoại hối', ',', 'tỷ giá', 'trung tâm', 'do', 'Ngân hàng', 'Nhà nước', 'công bố', 'sáng', 'nay', 'là', '22.743', 'đồng', 'một', 'USD', ',', 'tăng', '10', 'đồng', 'so', 'với', 'hôm qua', '.', 'Trong', 'khi', 'đó', ',', 'giá', 'USD', 'tại', 'các', 'ngân hàng', 'thương mại', 'lại', 'đi', 'xuống', '.', 'Cụ thể', ',', 'giá bán', 'đôla', 'Mỹ', 'tại', 'Vietcombank', 'hiện', 'quanh', '23.295', '-', '23.385', 'đồng', ',', 'giảm', '10', 'đồng', 'so', 'với', 'hôm qua', '.']\n"
     ]
    }
   ],
   "source": [
    "#better\n",
    "from underthesea import word_tokenize\n",
    "print(word_tokenize(text_vn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Chuyển text thành vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sử dụng hàm ```zip()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cô ấy', 'ấy không', 'không thích', 'thích trà']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def generate_ngrams_1(text, n):\n",
    "    text = text.lower()\n",
    "    tokens = re.split(r'[-.,?!;)()\\s]+', text)\n",
    "    tokens = [token for token in tokens if token!= \"\"]\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "s = \"cô ấy không thích trà\"\n",
    "generate_ngrams_1(s, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sử dụng nltk.util.ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cô ấy', 'ấy không', 'không thích', 'thích trà']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "def generate_ngrams_2(text, n):\n",
    "    text = text.lower()\n",
    "    tokens = re.split(r'[-.,?!;)()\\s]+', text)\n",
    "    tokens = [token for token in tokens if token!= \"\"]\n",
    "    output = list(ngrams(tokens, n))\n",
    "    output = [' '.join(i) for i in output]\n",
    "    return output\n",
    "s = \"cô ấy không thích trà\"\n",
    "generate_ngrams_2(s, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sử dụng CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?',\n",
    " ]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết hợp CountVectorizer với n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0]\n",
      " [0 0 2 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0]\n",
      " [1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0]\n",
      " [0 0 1 0 1 1 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1]]\n",
      "['and', 'and this', 'document', 'document is', 'first', 'first document', 'is', 'is the', 'is this', 'one', 'second', 'second document', 'the', 'the first', 'the second', 'the third', 'third', 'third one', 'this', 'this document', 'this is', 'this the']\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?',\n",
    " ]\n",
    "vectorizer_2 = CountVectorizer(ngram_range = (1, 2))\n",
    "Y = vectorizer_2.fit_transform(corpus)\n",
    "print(Y.toarray())\n",
    "print(vectorizer_2.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.        ]\n",
      "(1, 8)\n",
      "[[0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\n",
      "  0.36388646 0.42983441]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "\t\t\"The dog.\",\n",
    "\t\t\"The fox\"]\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)\n",
    "# encode document\n",
    "vector = vectorizer.transform([text[0]])\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Mô hình Logistic Regression cho bài toán phân loại"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dưới đây là quá trình xây dựng mô hình Logistic Regression để phân loại sắc thái bình luận (neg/pos) với dữ liệu là các comment trên twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 tải dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment                                      SentimentText\n",
       "0       1          0                       is so sad for my APL frie...\n",
       "1       2          0                     I missed the New Moon trail...\n",
       "2       3          1                            omg its already 7:30 :O\n",
       "3       4          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4       5          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/dhminh1024/practice_datasets/master/twitter_sentiment_analysis.csv', encoding='latin-1')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99989 entries, 0 to 99988\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   ItemID         99989 non-null  int64 \n",
      " 1   Sentiment      99989 non-null  int64 \n",
      " 2   SentimentText  99989 non-null  object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dùng cho tiếng Anh để nhóm các từ cùng kiểu vd: do, did, done thành một từ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 32880),\n",
       " ('the', 29630),\n",
       " ('to', 28812),\n",
       " ('you', 23471),\n",
       " ('a', 21321),\n",
       " ('i', 16000),\n",
       " ('and', 15826),\n",
       " ('it', 15228),\n",
       " ('my', 12385),\n",
       " ('for', 12378),\n",
       " ('in', 11199),\n",
       " ('is', 11185),\n",
       " ('that', 10920),\n",
       " ('have', 10566),\n",
       " ('of', 10326),\n",
       " ('on', 9021),\n",
       " ('but', 8489),\n",
       " ('me', 8255),\n",
       " (\"i'm\", 8167),\n",
       " ('be', 7944)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "vocab = Counter()\n",
    "for tweet in data.SentimentText:\n",
    "    tweet = tokenizer_porter(tweet)\n",
    "    for word in tweet:\n",
    "        vocab[word]+=1\n",
    "        \n",
    "# before processing\n",
    "vocab.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Loại bỏ từ không quan trọng "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Từ không quan trọng (stopword) là những từ xuất hiện nhiều nhưng nó hầu như không mang ý nghĩa (vd: the, is, that,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 32880),\n",
       " (\"i'm\", 8167),\n",
       " ('wa', 7137),\n",
       " ('get', 6470),\n",
       " ('like', 5646),\n",
       " ('go', 5498),\n",
       " ('love', 5003),\n",
       " ('good', 4955),\n",
       " ('-', 4922),\n",
       " ('thank', 4610),\n",
       " ('thi', 4364),\n",
       " (\"it'\", 4309),\n",
       " ('u', 4195),\n",
       " ('know', 3701),\n",
       " ('lol', 3505),\n",
       " ('see', 3349),\n",
       " ('one', 3188),\n",
       " ('think', 3129),\n",
       " ('hope', 3098),\n",
       " ('want', 3019)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_vocab= Counter()\n",
    "for word, c in vocab.items():\n",
    "    if word not in stop_words:\n",
    "        clean_vocab[word]=c\n",
    "# after cleaning\n",
    "clean_vocab.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Loại bỏ ký tự đặc biệt trừ emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like it  :)\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "def preprocessor(text):\n",
    "    \"\"\" Return a cleaned version of text\n",
    "    \"\"\"\n",
    "    # Remove HTML markup\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    # Save emoticons for later appending\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    # Remove any non-word character and append the emoticons,\n",
    "    # removing the nose character for standarization. Convert to lower case\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', ''))\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Create some random texts for testing the function preprocessor()\n",
    "print(preprocessor('I like it :), |||<><>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data['SentimentText']\n",
    "y = data['Sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=102)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sử dụng pipeline để kết hợp các bước từ số hóa dữ liệu -> tạo mô hình học từ dữ liệu\n",
    "\n",
    "Các bước thực hiện trong Pipeline bao gồm:\n",
    "- Loại bỏ stopwords\n",
    "- stemming các comment\n",
    "- loại bỏ ký tự đặc biệt trừ Emoji\n",
    "- Tạo vector tfidf \n",
    "- Huấn luyện Logistic Regression từ vector tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\softwares\\anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "D:\\softwares\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=<function prepr...e, penalty='l2', random_state=0, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=stop_words,\n",
    "                        tokenizer=tokenizer_porter,\n",
    "                        preprocessor=preprocessor)\n",
    "\n",
    "# A pipeline is what chains several steps together, once the initial exploration is done. \n",
    "# For example, some codes are meant to transform features — normalise numericals, or turn text into vectors, \n",
    "# or fill up missing data, they are transformers; other codes are meant to predict variables by fitting an algorithm,\n",
    "# they are estimators. Pipeline chains all these together which can then be applied to training data\n",
    "clf = Pipeline([('vect', tfidf),\n",
    "                ('clf', LogisticRegression(random_state=0))])\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Đánh giá mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7520252025202521\n",
      "confusion matrix:\n",
      " [[5814 2891]\n",
      " [2068 9225]]\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.67      0.70      8705\n",
      "           1       0.76      0.82      0.79     11293\n",
      "\n",
      "   micro avg       0.75      0.75      0.75     19998\n",
      "   macro avg       0.75      0.74      0.74     19998\n",
      "weighted avg       0.75      0.75      0.75     19998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Now apply those above metrics to evaluate your model\n",
    "# Your code here\n",
    "predictions = clf.predict(X_test)\n",
    "print('accuracy:',accuracy_score(y_test,predictions))\n",
    "print('confusion matrix:\\n',confusion_matrix(y_test,predictions))\n",
    "print('classification report:\\n',classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Dự đoán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not feel not bad --> Negative, Positive = [0.99056371 0.00943629]\n",
      "This model is \"so good\" :)) --> Negative, Positive = [0.16056502 0.83943498]\n",
      "we are who we are --> Negative, Positive = [0.38855991 0.61144009]\n",
      "its good to be bad sometimes --> Negative, Positive = [0.53694825 0.46305175]\n",
      "what a wonderful failure! (sarcasm :))) --> Negative, Positive = [0.30184744 0.69815256]\n",
      "People do not like the bad things --> Negative, Positive = [0.78125783 0.21874217]\n",
      "We finally have the test result. You are positive --> Negative, Positive = [0.20088749 0.79911251]\n"
     ]
    }
   ],
   "source": [
    "twits = [\n",
    "    \"I do not feel not bad\", # Phuc +1\n",
    "    'This model is \"so good\" :))', # Long -1\n",
    "    'we are who we are', # Nghi 0\n",
    "    'its good to be bad sometimes', # PA +1\n",
    "    'what a wonderful failure! (sarcasm :)))', #Phuc +1\n",
    "    'People do not like the bad things', # Chi 0\n",
    "    'We finally have the test result. You are positive', # Long +1\n",
    "]\n",
    "\n",
    "preds = clf.predict_proba(twits)\n",
    "\n",
    "for i in range(len(twits)):\n",
    "    print(f'{twits[i]} --> Negative, Positive = {preds[i]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
